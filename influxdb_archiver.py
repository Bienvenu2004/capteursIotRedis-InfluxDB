import influxdb_client
from influxdb_client.client.write_api import SYNCHRONOUS
from datetime import datetime, timedelta
import time
import subprocess
import json
import os

import generate_sharding_data


class InfluxDBArchiver:
    def __init__(self, token=None, url="http://localhost:8086", org="tp_iot", bucket="sensors_archive"):
        # Configuration
        self.url = url
        self.org = org
        self.bucket = bucket

        # Obtenir le token
        if token:
            self.token = token
        else:
            self.token = self.get_influxdb_token()

        print(f"\nConnexion √† InfluxDB avec les param√®tres:")
        print(f"URL: {self.url}")
        print(f"Token: {self.token[:5]}...{self.token[-3:] if len(self.token) > 8 else ''}")
        print(f"Org: {self.org}")
        print(f"Bucket: {self.bucket}")

        try:
            self.client = influxdb_client.InfluxDBClient(
                url=self.url,
                token=self.token,
                org=self.org,
                timeout=30_000
            )

            # Test de connexion par v√©rification de la sant√©
            health = self.client.health()
            if health and health.status == "pass":
                print(f"‚úÖ InfluxDB est en bonne sant√©: {health.status}")
            else:
                print(f"‚ö†Ô∏è InfluxDB sant√©: {health.status if health else 'inconnu'}")

            # Test de connexion par v√©rification des buckets
            try:
                buckets_api = self.client.buckets_api()
                buckets = buckets_api.find_buckets().buckets
                if buckets:
                    print(f"‚úÖ Buckets disponibles: {[b.name for b in buckets]}")
                else:
                    print("‚ö†Ô∏è Aucun bucket trouv√©")
            except Exception as bucket_error:
                print(f"‚ùå Erreur lors de la r√©cup√©ration des buckets: {bucket_error}")

            self.write_api = self.client.write_api(write_options=SYNCHRONOUS)
            self.query_api = self.client.query_api()

            print("‚úÖ Client InfluxDB initialis√© avec succ√®s")

        except Exception as e:
            print(f"‚ùå Erreur d'initialisation du client InfluxDB: {e}")
            raise

    def get_influxdb_token(self):
        """Tente de r√©cup√©rer automatiquement un token valide pour InfluxDB"""
        print("Tentative de r√©cup√©ration automatique du token InfluxDB...")

        # Option 1: Utiliser le token pr√©d√©fini
        default_token = "my-super-secret-tok"

        # Option 2: Essayer de r√©cup√©rer le token via Docker
        try:
            cmd = ["docker", "exec", "influxdb", "influx", "auth", "list", "--user", "admin", "-o", "tp_iot", "--json"]
            process = subprocess.run(cmd, capture_output=True, text=True)

            if process.returncode == 0 and process.stdout:
                try:
                    auth_data = json.loads(process.stdout)
                    if isinstance(auth_data, list) and len(auth_data) > 0:
                        # Utiliser le premier token trouv√© pour l'utilisateur admin
                        token = auth_data[0].get("token", "")
                        if token:
                            print("‚úÖ Token r√©cup√©r√© avec succ√®s via Docker!")
                            return token
                except json.JSONDecodeError:
                    print("‚ö†Ô∏è Format de r√©ponse non valide lors de la r√©cup√©ration du token")

            print("‚ö†Ô∏è Impossible de r√©cup√©rer le token via Docker")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'ex√©cution de la commande Docker: {e}")

        # Option 3: Demander √† l'utilisateur
        user_token = input(f"Entrez le token InfluxDB manuellement (laissez vide pour utiliser '{default_token}'): ")
        if user_token:
            return user_token

        print(f"Utilisation du token par d√©faut: {default_token[:5]}...")
        return default_token

    def ensure_bucket_exists(self):
        """S'assure que le bucket existe, le cr√©e si n√©cessaire"""
        try:
            bucket_api = self.client.buckets_api()
            bucket = bucket_api.find_bucket_by_name(self.bucket)
            if bucket:
                print(f"‚úÖ Le bucket '{self.bucket}' existe")
                return True

            print(f"‚ö†Ô∏è Le bucket '{self.bucket}' n'existe pas. Tentative de cr√©ation...")
            orgs = self.client.organizations_api().find_organizations()
            if not orgs:
                print("‚ùå Aucune organisation trouv√©e")
                return False

            org_id = orgs[0].id
            bucket_api.create_bucket(bucket_name=self.bucket, org_id=org_id)
            print(f"‚úÖ Bucket '{self.bucket}' cr√©√© avec succ√®s")
            return True
        except Exception as e:
            print(f"‚ùå Erreur lors de la v√©rification/cr√©ation du bucket: {e}")
            return False

    def archive_redis_data(self, redis_system, days_back=7):
        """Archive les donn√©es Redis vers InfluxDB"""
        try:
            if not self.ensure_bucket_exists():
                print("‚ùå Impossible de continuer l'archivage sans bucket valide")
                return False

            end_time = datetime.now()
            start_time = end_time - timedelta(days=days_back)
            print(f"\nArchivage des donn√©es du {start_time} au {end_time}...")

            points = []
            for location in redis_system.locations:
                if location not in redis_system.connections:
                    continue

                conn = redis_system.get_connection_for_location(location)
                for sensor_type in redis_system.sensor_types:
                    for sensor_id in range(1, redis_system.num_sensors + 1):
                        key = f"sensor:{sensor_type}:{location}:{sensor_id}"
                        try:
                            data = conn.execute_command("TS.RANGE", key,
                                                        int(start_time.timestamp() * 1000),
                                                        int(end_time.timestamp() * 1000))
                            for timestamp_ms, value in data:
                                point = influxdb_client.Point(sensor_type) \
                                    .tag("location", location) \
                                    .tag("sensor_id", str(sensor_id)) \
                                    .field("value", float(value)) \
                                    .time(int(timestamp_ms) * 1_000_000)  # Conversion ms -> ns
                                points.append(point)

                        except Exception as e:
                            print(f"‚ö†Ô∏è Erreur sur {key}: {str(e)[:100]}...")

            if points:
                print(f"Tentative d'√©criture de {len(points)} points...")
                try:
                    self.write_api.write(bucket=self.bucket, org=self.org, record=points)
                    print("‚úÖ Donn√©es √©crites avec succ√®s")

                    # V√©rification
                    query = f'from(bucket:"{self.bucket}") |> range(start:-1h) |> limit(n:1)'
                    result = self.query_api.query(query)
                    print(f"V√©rification : {len(result)} tables trouv√©es")

                    return True
                except Exception as write_error:
                    print(f"‚ùå Erreur d'√©criture: {write_error}")
                    return False
            else:
                print("‚ö†Ô∏è Aucune donn√©e √† archiver")
                return True

        except Exception as e:
            print(f"‚ùå Erreur lors de l'archivage: {e}")
            return False


def main():
    try:
        # Initialisation du syst√®me Redis
        print("Initialisation du syst√®me Redis de True Sharding...")
        redis_system = generate_sharding_data.RedisTrueShardingSystem()

        # Initialisation de l'archiveur InfluxDB avec r√©cup√©ration automatique du token
        influx_archiver = InfluxDBArchiver()

        # Test initial d'archivage
        print("\nTest initial d'archivage (1 jour de donn√©es)...")
        success = influx_archiver.archive_redis_data(redis_system, days_back=1)

        if not success:
            print("\n‚ùå Le test initial d'archivage a √©chou√©. V√©rifiez les param√®tres et recommencez.")
            return

        print("\n‚úÖ Test initial r√©ussi! D√©marrage de la boucle principale...")
        print("Appuyez sur Ctrl+C pour arr√™ter le programme")

        # Boucle principale
        try:
            while True:
                # G√©n√©rer des donn√©es en direct
                redis_system.generate_live_data(interval_seconds=30)

                # Archivage p√©riodique toutes les 5 minutes
                current_time = datetime.now()
                if current_time.minute % 5 == 0 and current_time.second < 30:
                    print(f"\n‚è∞ {current_time} - Archivage p√©riodique...")
                    influx_archiver.archive_redis_data(redis_system, days_back=0.1)  # ~2.4 heures
                    time.sleep(60)  # √âviter les doubles ex√©cutions

        except KeyboardInterrupt:
            print("\n\nüõë Interruption d√©tect√©e. Arr√™t propre du syst√®me...")

    except KeyboardInterrupt:
        print("\nüõë Arr√™t du syst√®me")
    except Exception as e:
        print(f"\n‚ùå Erreur critique: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()